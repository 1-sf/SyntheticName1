{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "from ner_post_processing import parse_entities_promptner, get_token_labels\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vkmr/miniforge3/envs/mach/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "Found cached dataset cross_ner (/Users/vkmr/.cache/huggingface/datasets/DFKI-SLT___cross_ner/politics/1.1.0/e1d1a6ac35c3ee9d62d89789aad42c65e8266eb7d75bcba812d59e45639c005e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adc5d47dcf42498fa04e40b9a80e39eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "dataset = load_dataset(\"DFKI-SLT/cross_ner\", \"politics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = dataset[\"validation\"].features[\"ner_tags\"].feature.names\n",
    "index2label = {i: label for i, label in enumerate(class_labels)}\n",
    "label2index = {v: k for k, v in index2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "class_labels = dataset[\"validation\"].features[\"ner_tags\"].feature.names\n",
    "index2label = {i: label for i, label in enumerate(class_labels)}\n",
    "label2index = {v: k for k, v in index2label.items()}\n",
    "\n",
    "def score_ner(prediction_batch,gold_batch):\n",
    "    labelled_predicions = []\n",
    "    for prediction in prediction_batch:\n",
    "        labelled_predicions.append([index2label[i] for i in prediction])\n",
    "    labelled_gold = []\n",
    "    for gold in gold_batch:\n",
    "        labelled_gold.append([index2label[i] for i in gold])\n",
    "    return metric.compute(\n",
    "    predictions=labelled_predicions, \n",
    "    references=labelled_gold\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataset[\"test\"])\n",
    "instruction = \"An entity is a person (person), organization (organization), politician (politician), political party (politicalparty), event (event), election (election), country (country), location (location), or other political entity (misc). Dates, times, abstract concepts, adjectives, and verbs are not entities.\\n\\nFor each potential entity in the text, determine if it is an entity and, if so, its type. Provide the reason for your decision. Format your response as a YAML list, with each item containing the following fields:\\n\\nspan: The text span of the potential entity.\\nentity_type: The type of the entity (person, organization, politician, politicalparty, event, election, country, location, misc) or false if not an entity.\\nreason: A brief explanation of why the span is or is not an entity.\",\n",
    "df[\"inference_prompt\"] = df.apply(lambda x:f\"### INSTRUCTION: {instruction} ### PARAGRAPH: {x['tokens']}  ### TAG_SPANS: \",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request failed. Retrying in 1 second(s)...\n",
      "Request failed. Retrying in 1 second(s)...\n",
      "Request failed. Retrying in 1 second(s)...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import concurrent.futures\n",
    "import time\n",
    "\n",
    "def generate_completion(model_id, prompt, retries=3, delay=1):\n",
    "    url = \"https://api.fireworks.ai/inference/v1/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": \"Bearer FFJxiShwuQO0MyRm7ynfQnDkWdZYosEBIOVEf2AbIyzyAXre\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": f\"accounts/vaibhavk992-6442ca/models/{model_id}\",\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": 32768,\n",
    "    }\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.post(url, headers=headers, json=data)\n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                return result\n",
    "            else:\n",
    "                raise Exception(f\"Request failed with status code: {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            if attempt < retries - 1:\n",
    "                print(f\"Request failed. Retrying in {delay} second(s)...\")\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "def process_row(row):\n",
    "    prompt = row[\"inference_prompt\"]\n",
    "    result = generate_completion(\"76c17b1df47749e3bca1c98f027a672b\", prompt)\n",
    "    return result\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=100) as executor:\n",
    "    futures = []\n",
    "    for _, row in df.iterrows():\n",
    "        future = executor.submit(process_row, row)\n",
    "        futures.append(future)\n",
    "\n",
    "    results = [future.result() for future in concurrent.futures.as_completed(futures)]\n",
    "\n",
    "df[\"model_raw_output\"] = results\n",
    "df[\"filtered_output\"] = df[\"model_raw_output\"].apply(lambda x: x[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def apply_safe_json(x):\n",
    "    try:\n",
    "        return json.loads(x)\n",
    "    except:\n",
    "        return []\n",
    "df[\"json_output\"] = df[\"filtered_output\"].apply(apply_safe_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "651it [00:00, 15395.89it/s]\n",
      "/Users/vkmr/miniforge3/envs/mach/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'country': {'precision': 0.4117647058823529,\n",
       "  'recall': 0.01674641148325359,\n",
       "  'f1': 0.03218390804597702,\n",
       "  'number': 418},\n",
       " 'election': {'precision': 0.6153846153846154,\n",
       "  'recall': 0.018433179723502304,\n",
       "  'f1': 0.035794183445190156,\n",
       "  'number': 434},\n",
       " 'event': {'precision': 0.4666666666666667,\n",
       "  'recall': 0.035897435897435895,\n",
       "  'f1': 0.06666666666666665,\n",
       "  'number': 195},\n",
       " 'location': {'precision': 0.46153846153846156,\n",
       "  'recall': 0.02003338898163606,\n",
       "  'f1': 0.038400000000000004,\n",
       "  'number': 599},\n",
       " 'misc': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 258},\n",
       " 'organisation': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 513},\n",
       " 'person': {'precision': 0.21428571428571427,\n",
       "  'recall': 0.00847457627118644,\n",
       "  'f1': 0.016304347826086956,\n",
       "  'number': 354},\n",
       " 'politicalparty': {'precision': 0.5517241379310345,\n",
       "  'recall': 0.016789087093389297,\n",
       "  'f1': 0.032586558044806514,\n",
       "  'number': 953},\n",
       " 'politician': {'precision': 1.0,\n",
       "  'recall': 0.004123711340206186,\n",
       "  'f1': 0.008213552361396304,\n",
       "  'number': 485},\n",
       " 'overall_precision': 0.4296875,\n",
       " 'overall_recall': 0.013067236873366595,\n",
       " 'overall_f1': 0.025363154254092694,\n",
       " 'overall_accuracy': 0.6318288925140475}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scored = defaultdict(list)\n",
    "\n",
    "def parse_entities_finetune(output):\n",
    "    entities = []\n",
    "    for entry in output:\n",
    "        try:\n",
    "            entity = entry[\"span\"]\n",
    "            is_entity = entry[\"entity_type\"] != 'false'\n",
    "            reasoning = entry[\"reason\"]\n",
    "            tag = entry[\"entity_type\"]\n",
    "            entities.append((entity, is_entity, reasoning, tag))\n",
    "        except:\n",
    "            continue\n",
    "    return entities\n",
    "\n",
    "\n",
    "for idx, example in tqdm(df.iterrows()):\n",
    "    try:\n",
    "        text = \" \".join(example[\"tokens\"])\n",
    "        ner_tags = get_token_labels(text, parse_entities_finetune(example[\"json_output\"]), label2index)\n",
    "        scored[\"id\"].append(example[\"id\"])\n",
    "        scored[\"tokens\"].append(example[\"tokens\"])\n",
    "        scored[\"ner_tags\"].append(ner_tags)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "\n",
    "df_scored = pd.DataFrame(scored)\n",
    "score_ner(df_scored[\"ner_tags\"].to_list(), dataset[\"test\"][\"ner_tags\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mach",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
