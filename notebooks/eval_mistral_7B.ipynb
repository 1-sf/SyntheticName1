{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dachun/miniconda3/envs/mistral-ai-hackathon/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "from mistral_client import run_mistral\n",
    "from ner_post_processing import parse_entities_promptner, get_token_labels\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"DFKI-SLT/cross_ner\", \"politics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-academicjournal': 1,\n",
       " 'I-academicjournal': 2,\n",
       " 'B-album': 3,\n",
       " 'I-album': 4,\n",
       " 'B-algorithm': 5,\n",
       " 'I-algorithm': 6,\n",
       " 'B-astronomicalobject': 7,\n",
       " 'I-astronomicalobject': 8,\n",
       " 'B-award': 9,\n",
       " 'I-award': 10,\n",
       " 'B-band': 11,\n",
       " 'I-band': 12,\n",
       " 'B-book': 13,\n",
       " 'I-book': 14,\n",
       " 'B-chemicalcompound': 15,\n",
       " 'I-chemicalcompound': 16,\n",
       " 'B-chemicalelement': 17,\n",
       " 'I-chemicalelement': 18,\n",
       " 'B-conference': 19,\n",
       " 'I-conference': 20,\n",
       " 'B-country': 21,\n",
       " 'I-country': 22,\n",
       " 'B-discipline': 23,\n",
       " 'I-discipline': 24,\n",
       " 'B-election': 25,\n",
       " 'I-election': 26,\n",
       " 'B-enzyme': 27,\n",
       " 'I-enzyme': 28,\n",
       " 'B-event': 29,\n",
       " 'I-event': 30,\n",
       " 'B-field': 31,\n",
       " 'I-field': 32,\n",
       " 'B-literarygenre': 33,\n",
       " 'I-literarygenre': 34,\n",
       " 'B-location': 35,\n",
       " 'I-location': 36,\n",
       " 'B-magazine': 37,\n",
       " 'I-magazine': 38,\n",
       " 'B-metrics': 39,\n",
       " 'I-metrics': 40,\n",
       " 'B-misc': 41,\n",
       " 'I-misc': 42,\n",
       " 'B-musicalartist': 43,\n",
       " 'I-musicalartist': 44,\n",
       " 'B-musicalinstrument': 45,\n",
       " 'I-musicalinstrument': 46,\n",
       " 'B-musicgenre': 47,\n",
       " 'I-musicgenre': 48,\n",
       " 'B-organisation': 49,\n",
       " 'I-organisation': 50,\n",
       " 'B-person': 51,\n",
       " 'I-person': 52,\n",
       " 'B-poem': 53,\n",
       " 'I-poem': 54,\n",
       " 'B-politicalparty': 55,\n",
       " 'I-politicalparty': 56,\n",
       " 'B-politician': 57,\n",
       " 'I-politician': 58,\n",
       " 'B-product': 59,\n",
       " 'I-product': 60,\n",
       " 'B-programlang': 61,\n",
       " 'I-programlang': 62,\n",
       " 'B-protein': 63,\n",
       " 'I-protein': 64,\n",
       " 'B-researcher': 65,\n",
       " 'I-researcher': 66,\n",
       " 'B-scientist': 67,\n",
       " 'I-scientist': 68,\n",
       " 'B-song': 69,\n",
       " 'I-song': 70,\n",
       " 'B-task': 71,\n",
       " 'I-task': 72,\n",
       " 'B-theory': 73,\n",
       " 'I-theory': 74,\n",
       " 'B-university': 75,\n",
       " 'I-university': 76,\n",
       " 'B-writer': 77,\n",
       " 'I-writer': 78}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_labels = dataset[\"validation\"].features[\"ner_tags\"].feature.names\n",
    "index2label = {i: label for i, label in enumerate(class_labels)}\n",
    "label2index = {v: k for k, v in index2label.items()}\n",
    "\n",
    "label2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 541\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 651\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  You are an expert linguist. Your task \n",
    " \n",
    "prompt = lambda text: f\"\"\"\n",
    "Dfn: An entity is a person (person), organisation (organisation), politician (politician), political party (politicalparty), event (event), election (election), \n",
    "country (country), location (location) or other political entity (misc). Dates, times, abstract concepts, adjectives, and verbs are not entities.\n",
    "\n",
    "Example 1: Sitting as a Liberal Party of Canada Member of Parliament (MP) for Niagara Falls, she joined the Canadian Cabinet after the Liberals defeated the \n",
    "Progressive Conservative Party of Canada government of John Diefenbaker in the 1963 Canadian federal election.\n",
    "\n",
    "Answer:\n",
    "1. Liberal Party of Canada | True | as it is a political party (politicalparty)\n",
    "2. Parliament | True | as it is an organisation (organisation)\n",
    "3. Niagara Falls | True | as it is a location (location)\n",
    "4. Canadian Cabinet | True | as it is a political entity (misc)\n",
    "5. Liberals | True | as it is a political group by not the party name (misc)\n",
    "6. Progressive Conservative Party of Canada | True | as it is a political party (politicalparty)\n",
    "7. government | False | as it is not actually an entity in this sentence\n",
    "8. John Diefenbaker | True | as it is a politician (politician)\n",
    "9. 1963 Canadian federal election | True | as it is an election (election)\n",
    "\n",
    "Example 2: The MRE took part to the consolidation of The Olive Tree as a joint electoral list both for the\n",
    "2004 European Parliament election and the 2006 Italian general election, along with the Democrats of the Left\n",
    "and Democracy is Freedom - The Daisy.\n",
    "\n",
    "Answer:\n",
    "1. MRE | True | as it is a political party (politicalparty)\n",
    "2. consolidation | False | as it is an action\n",
    "3. The Olive Tree | True | as it is a group or organisation (organisation)\n",
    "4. 2004 European Parliament election | True | as it is an election (election)\n",
    "5. 2006 Italian general election | True | as it is an election (election)\n",
    "6. Democrats of the Left | True | as it is a political party (politicalparty)\n",
    "7. Democracy is Freedom - The Daisy | True | as it is an political party (politicalparty)\n",
    "\n",
    "Q. Given the paragraph below, identify a list of possible entities and for each entry explain why it either is or is not an entity.\n",
    "\n",
    "Paragraph: {text}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "def score_ner(prediction_batch, gold_batch):\n",
    "    labeled_predicions = []\n",
    "    for prediction in prediction_batch:\n",
    "        labeled_predicions.append([index2label[i] for i in prediction])\n",
    "    labeled_gold = []\n",
    "    for gold in gold_batch:\n",
    "        labeled_gold.append([index2label[i] for i in gold])\n",
    "    return metric.compute(\n",
    "        predictions=labeled_predicions, \n",
    "        references=labeled_gold\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 651/651 [32:46<00:00,  3.02s/it]\n"
     ]
    }
   ],
   "source": [
    "scored = defaultdict(list)\n",
    "\n",
    "for idx, example in enumerate(tqdm(dataset[\"test\"])):\n",
    "    if (idx + 1) % 100 == 0:\n",
    "        df_scored = pd.DataFrame(scored)\n",
    "        df_scored.to_csv(\"../data/scored/test.mistral_7b.csv\", index=False)\n",
    "    \n",
    "    try:\n",
    "        text = \" \".join(example[\"tokens\"])\n",
    "        prompt_input = prompt(text)\n",
    "        output = run_mistral(prompt_input, model=\"open-mistral-7b\")\n",
    "        ner_tags = get_token_labels(text, parse_entities_promptner(output), label2index)\n",
    "\n",
    "        scored[\"id\"].append(example[\"id\"])\n",
    "        scored[\"tokens\"].append(example[\"tokens\"])\n",
    "        scored[\"prompt\"].append(prompt_input)\n",
    "        scored[\"output\"].append(output)\n",
    "        scored[\"ner_tags\"].append(ner_tags)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "\n",
    "df_scored = pd.DataFrame(scored)\n",
    "df_scored.to_csv(\"../data/scored/test.mistral_7b.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dachun/miniconda3/envs/mistral-ai-hackathon/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'country': {'precision': 0.6782608695652174,\n",
       "  'recall': 0.37320574162679426,\n",
       "  'f1': 0.4814814814814814,\n",
       "  'number': 418},\n",
       " 'election': {'precision': 0.8169491525423729,\n",
       "  'recall': 0.5552995391705069,\n",
       "  'f1': 0.6611796982167353,\n",
       "  'number': 434},\n",
       " 'event': {'precision': 0.6296296296296297,\n",
       "  'recall': 0.4358974358974359,\n",
       "  'f1': 0.5151515151515151,\n",
       "  'number': 195},\n",
       " 'location': {'precision': 0.7574257425742574,\n",
       "  'recall': 0.5108514190317195,\n",
       "  'f1': 0.6101694915254237,\n",
       "  'number': 599},\n",
       " 'misc': {'precision': 0.07124681933842239,\n",
       "  'recall': 0.10852713178294573,\n",
       "  'f1': 0.08602150537634408,\n",
       "  'number': 258},\n",
       " 'organisation': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 513},\n",
       " 'person': {'precision': 0.5452380952380952,\n",
       "  'recall': 0.6468926553672316,\n",
       "  'f1': 0.5917312661498707,\n",
       "  'number': 354},\n",
       " 'politicalparty': {'precision': 0.7504215851602024,\n",
       "  'recall': 0.4669464847848898,\n",
       "  'f1': 0.5756791720569211,\n",
       "  'number': 953},\n",
       " 'politician': {'precision': 0.7534883720930232,\n",
       "  'recall': 0.334020618556701,\n",
       "  'f1': 0.46285714285714286,\n",
       "  'number': 485},\n",
       " 'overall_precision': 0.615270018621974,\n",
       " 'overall_recall': 0.3924922784509385,\n",
       " 'overall_f1': 0.4792573252103278,\n",
       " 'overall_accuracy': 0.7671923146637666}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_ner(df_scored[\"ner_tags\"].to_list(), dataset[\"test\"][\"ner_tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 541/541 [51:51<00:00,  5.75s/it]  \n"
     ]
    }
   ],
   "source": [
    "scored = defaultdict(list)\n",
    "\n",
    "for idx, example in enumerate(tqdm(dataset[\"validation\"])):\n",
    "    if (idx + 1) % 100 == 0:\n",
    "        df_scored = pd.DataFrame(scored)\n",
    "        df_scored.to_csv(\"../data/scored/validation.mistral_7b.csv\", index=False)\n",
    "    \n",
    "    text = \" \".join(example[\"tokens\"])\n",
    "    prompt_input = prompt(text)\n",
    "    output = run_mistral(prompt_input)\n",
    "    ner_tags = get_token_labels(text, parse_entities_promptner(output), label2index)\n",
    "\n",
    "    scored[\"id\"].append(example[\"id\"])\n",
    "    scored[\"tokens\"].append(example[\"tokens\"])\n",
    "    scored[\"prompt\"].append(prompt_input)\n",
    "    scored[\"output\"].append(output)\n",
    "    scored[\"ner_tags\"].append(ner_tags)\n",
    "\n",
    "df_scored = pd.DataFrame(scored)\n",
    "df_scored.to_csv(\"../data/scored/validation.mistral_7b.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'country': {'precision': 0.6416184971098265,\n",
       "  'recall': 0.6065573770491803,\n",
       "  'f1': 0.6235955056179776,\n",
       "  'number': 183},\n",
       " 'election': {'precision': 0.783303730017762,\n",
       "  'recall': 0.8352272727272727,\n",
       "  'f1': 0.8084326306141154,\n",
       "  'number': 528},\n",
       " 'event': {'precision': 0.4648648648648649,\n",
       "  'recall': 0.46236559139784944,\n",
       "  'f1': 0.46361185983827496,\n",
       "  'number': 186},\n",
       " 'location': {'precision': 0.4962121212121212,\n",
       "  'recall': 0.6208530805687204,\n",
       "  'f1': 0.5515789473684211,\n",
       "  'number': 211},\n",
       " 'misc': {'precision': 0.1050228310502283,\n",
       "  'recall': 0.11917098445595854,\n",
       "  'f1': 0.1116504854368932,\n",
       "  'number': 193},\n",
       " 'organisation': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 431},\n",
       " 'person': {'precision': 0.6333333333333333,\n",
       "  'recall': 0.7972027972027972,\n",
       "  'f1': 0.7058823529411764,\n",
       "  'number': 286},\n",
       " 'politicalparty': {'precision': 0.6825028968713789,\n",
       "  'recall': 0.5593542260208927,\n",
       "  'f1': 0.6148225469728602,\n",
       "  'number': 1053},\n",
       " 'politician': {'precision': 0.6957831325301205,\n",
       "  'recall': 0.5620437956204379,\n",
       "  'f1': 0.6218034993270525,\n",
       "  'number': 411},\n",
       " 'overall_precision': 0.6218316998986144,\n",
       " 'overall_recall': 0.5284319356691557,\n",
       " 'overall_f1': 0.5713398540599286,\n",
       " 'overall_accuracy': 0.8229775828460039}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_ner(df_scored[\"ner_tags\"].to_list(), dataset[\"validation\"][\"ner_tags\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral-ai-hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
